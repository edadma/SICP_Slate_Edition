\section{Procedures and the Processes They Generate}

We have now considered the elements of programming: We have used primitive arithmetic operations, we have combined these operations, and we have abstracted these composite operations by defining them as compound procedures. But that is not enough to enable us to say that we know how to program. Our situation is analogous to that of someone who has learned the rules for how the pieces move in chess but knows nothing of typical openings, tactics, or strategy. Like the novice chess player, we don't yet know the common patterns of usage in the domain. We lack the knowledge of which moves are worth making (which procedures are worth defining). We lack the experience to predict the consequences of making a move (executing a procedure).

The ability to visualize the consequences of the actions under consideration is crucial to becoming an expert programmer, just as it is in any synthetic, creative activity. In becoming an expert photographer, for example, one must learn how to look at a scene and know how dark each region will appear on a print for each possible choice of exposure and development conditions. Only then can one reason backward, planning framing, lighting, exposure, and development to obtain the desired effects. So it is with programming, where we are planning the course of action to be taken by a process and where we control the process by means of a program. To become experts, we must learn to visualize the processes generated by various types of procedures. Only after we have developed such a skill can we learn to reliably construct programs that exhibit the desired behavior.

A procedure is a pattern for the local evolution of a computational process. It specifies how each stage of the process is built upon the previous stage. We would like to be able to make statements about the overall, or global, behavior of a process whose local evolution has been specified by a procedure. This is very difficult to do in general, but we can at least try to describe some typical patterns of process evolution.

In this section we will examine some common ``shapes'' for processes generated by simple procedures. We will also investigate the rates at which these processes consume the important computational resources of time and space. The procedures we will consider are very simple. Their role is like that played by test patterns in photography: as oversimplified prototypical patterns, rather than practical examples in their own right.

\subsection{Linear Recursion and Iteration}

We begin by considering the factorial function, defined by

$n! = n \cdot (n - 1) \cdot (n - 2) \cdots 3 \cdot 2 \cdot 1$.

There are many ways to compute factorials. One way is to make use of the observation that $n!$ is equal to $n$ times $(n - 1)!$ for any positive integer $n$:

$n! = n \cdot [(n - 1) \cdot (n - 2) \cdots 3 \cdot 2 \cdot 1] = n \cdot (n - 1)!$.

Thus, we can compute $n!$ by computing $(n - 1)!$ and multiplying the result by $n$. If we add the stipulation that $1!$ is equal to 1, this observation translates directly into a procedure:

\begin{lstlisting}
def factorial(n) =
    if n == 1 then
        1
    else
        n * factorial(n - 1)
\end{lstlisting}

We can use the substitution model of Section 1.1.5 to watch this procedure in action computing $6!$, as shown in Figure~\ref{fig:linear-recursive-factorial}:

\begin{verbatim}
factorial(6)
6 * factorial(5)
6 * (5 * factorial(4))
6 * (5 * (4 * factorial(3)))
6 * (5 * (4 * (3 * factorial(2))))
6 * (5 * (4 * (3 * (2 * factorial(1)))))
6 * (5 * (4 * (3 * (2 * 1))))
6 * (5 * (4 * (3 * 2)))
6 * (5 * (4 * 6))
6 * (5 * 24)
6 * 120
720
\end{verbatim}

\begin{figure}[h]
\centering
\caption{A linear recursive process for computing $6!$.}
\label{fig:linear-recursive-factorial}
\end{figure}

Now let's take a different perspective on computing factorials. We could describe a rule for computing $n!$ by specifying that we first multiply 1 by 2, then multiply the result by 3, then by 4, and so on until we reach $n$. More formally, we maintain a running product, together with a counter that counts from 1 up to $n$. We can describe the computation by saying that the counter and the product simultaneously change from one step to the next according to the rule

$\text{product} \leftarrow \text{counter} \times \text{product}$
$\text{counter} \leftarrow \text{counter} + 1$

and stipulating that $n!$ is the value of the product when the counter exceeds $n$.

Once again, we can recast our description as a procedure for computing factorials:

\begin{lstlisting}
def factorial(n) = fact_iter(1, 1, n)

def fact_iter(product, counter, max_count) =
    if counter > max_count then
        product
    else
        fact_iter(counter * product, counter + 1, max_count)
\end{lstlisting}

As before, we can use the substitution model to visualize the process of computing $6!$, as shown in Figure~\ref{fig:linear-iterative-factorial}:

\begin{verbatim}
factorial(6)
fact_iter(1, 1, 6)
fact_iter(1, 2, 6)
fact_iter(2, 3, 6)
fact_iter(6, 4, 6)
fact_iter(24, 5, 6)
fact_iter(120, 6, 6)
fact_iter(720, 7, 6)
720
\end{verbatim}

\begin{figure}[h]
\centering
\caption{A linear iterative process for computing $6!$.}
\label{fig:linear-iterative-factorial}
\end{figure}

Compare the two processes. From one point of view, they seem hardly different at all. Both compute the same mathematical function on the same domain, and each requires a number of steps proportional to $n$ to compute $n!$. Indeed, both processes even carry out the same sequence of multiplications, obtaining the same sequence of partial products. On the other hand, when we consider the ``shapes'' of the two processes, we find that they evolve quite differently.

Consider the first process. The substitution model reveals a shape of expansion followed by contraction, indicated by the arrow in Figure~\ref{fig:linear-recursive-factorial}. The expansion occurs as the process builds up a chain of deferred operations (in this case, a chain of multiplications). The contraction occurs as the operations are actually performed. This type of process, characterized by a chain of deferred operations, is called a \textit{recursive process}. Carrying out this process requires that the interpreter keep track of the operations to be performed later on. In the computation of $n!$, the length of the chain of deferred multiplications, and hence the amount of information needed to keep track of it, grows linearly with $n$ (is proportional to $n$), just like the number of steps. Such a process is called a \textit{linear recursive process}.

By contrast, the second process does not grow and shrink. At each step, all we need to keep track of, for any $n$, are the current values of the variables \slate{product}, \slate{counter}, and \slate{max_count}. We call this an \textit{iterative process}. In general, an iterative process is one whose state can be summarized by a fixed number of state variables, together with a fixed rule that describes how the state variables should be updated as the process moves from state to state and an (optional) end test that specifies conditions under which the process should terminate. In computing $n!$, the number of steps required grows linearly with $n$. Such a process is called a \textit{linear iterative process}.

The contrast between the two processes can be seen in another way. In the iterative case, the program variables provide a complete description of the state of the process at any point. If we stopped the computation between steps, all we would need to do to resume the computation is to supply the interpreter with the values of the three program variables. Not so with the recursive process. In this case there is some additional ``hidden'' information, maintained by the interpreter and not contained in the program variables, which indicates ``where the process is'' in negotiating the chain of deferred operations. The longer the chain, the more information must be maintained.

In contrasting iteration and recursion, we must be careful not to confuse the notion of a recursive process with the notion of a recursive procedure. When we describe a procedure as recursive, we are referring to the syntactic fact that the procedure definition refers (either directly or indirectly) to the procedure itself. But when we describe a process as following a pattern that is, say, linearly recursive, we are speaking about how the process evolves, not about the syntax of how a procedure is written. It may seem disturbing that we refer to a recursive procedure such as \slate{fact_iter} as generating an iterative process. However, the process really is iterative: Its state is captured completely by its three state variables, and an interpreter need keep track of only three variables in order to execute the process.

One reason that the distinction between process and procedure may be confusing is that most implementations of common languages (including Ada, Pascal, and C) are designed in such a way that the interpretation of any recursive procedure consumes an amount of memory that grows with the number of procedure calls, even when the process described is, in principle, iterative. As a consequence, these languages can describe iterative processes only by resorting to special-purpose ``looping constructs'' such as \slate{do}, \slate{repeat}, \slate{until}, \slate{for}, and \slate{while}. The implementation of Slate we shall consider does not share this defect. It will execute an iterative process in constant space, even if the iterative process is described by a recursive procedure. An implementation with this property is called \textit{tail-recursive}. With a tail-recursive implementation, iteration can be expressed using the ordinary procedure call mechanism, so that special iteration constructs are useful only as syntactic sugar.

\begin{description}
\item[Exercise 1.9] Each of the following two procedures defines a method for adding two positive integers in terms of the procedures \slate{inc}, which increments its argument by 1, and \slate{dec}, which decrements its argument by 1.

\begin{lstlisting}
def add_v1(a, b) =
    if a == 0 then b else inc(add_v1(dec(a), b))

def add_v2(a, b) =
    if a == 0 then b else add_v2(dec(a), inc(b))
\end{lstlisting}

Using the substitution model, illustrate the process generated by each procedure in evaluating \slate{add_v1(4, 5)} and \slate{add_v2(4, 5)}. Are these processes iterative or recursive?

\item[Exercise 1.10] The following procedure computes a mathematical function called Ackermann's function.

\begin{lstlisting}
def A(x, y) =
    if y == 0 then 0
    elif x == 0 then 2 * y
    elif y == 1 then 2
    else A(x - 1, A(x, y - 1))
\end{lstlisting}

What are the values of the following expressions?

\begin{lstlisting}
A(1, 10)
A(2, 4)
A(3, 3)
\end{lstlisting}

Consider the following procedures, where \slate{A} is the procedure defined above:

\begin{lstlisting}
def f(n) = A(0, n)
def g(n) = A(1, n)
def h(n) = A(2, n)
def k(n) = 5 * n * n
\end{lstlisting}

Give concise mathematical definitions for the functions computed by the procedures \slate{f}, \slate{g}, and \slate{h} for positive integer values of $n$. For example, \slate{k(n)} computes $5n^2$.
\end{description}

\subsection{Tree Recursion}

Another common pattern of computation is called \textit{tree recursion}. As an example, consider computing the sequence of Fibonacci numbers, in which each number is the sum of the preceding two:

$0, 1, 1, 2, 3, 5, 8, 13, 21, \ldots$

In general, the Fibonacci numbers can be defined by the rule

\begin{equation}
\text{Fib}(n) = \begin{cases}
0 & \text{if } n = 0, \\
1 & \text{if } n = 1, \\
\text{Fib}(n - 1) + \text{Fib}(n - 2) & \text{otherwise}.
\end{cases}
\end{equation}

We can immediately translate this definition into a recursive procedure for computing Fibonacci numbers:

\begin{lstlisting}
def fib(n) =
    if n == 0 then 0
    elif n == 1 then 1
    else fib(n - 1) + fib(n - 2)
\end{lstlisting}

Consider the pattern of this computation. To compute \slate{fib(5)}, we compute \slate{fib(4)} and \slate{fib(3)}. To compute \slate{fib(4)}, we compute \slate{fib(3)} and \slate{fib(2)}. In general, the evolved process looks like a tree, as shown in Figure~\ref{fig:tree-recursive-fib}. Notice that the branches split into two at each level (except at the bottom); this reflects the fact that the \slate{fib} procedure calls itself twice each time it is invoked.

\begin{figure}[h]
\centering
\begin{tikzpicture}[level distance=1.2cm,
                    level 1/.style={sibling distance=4cm},
                    level 2/.style={sibling distance=2cm},
                    level 3/.style={sibling distance=1cm},
                    level 4/.style={sibling distance=0.6cm}]
\node {\slate{fib(5)}}
    child {node {\slate{fib(4)}}
        child {node {\slate{fib(3)}}
            child {node {\slate{fib(2)}}
                child {node {\slate{fib(1)}}}
                child {node {\slate{fib(0)}}}
            }
            child {node {\slate{fib(1)}}}
        }
        child {node {\slate{fib(2)}}
            child {node {\slate{fib(1)}}}
            child {node {\slate{fib(0)}}}
        }
    }
    child {node {\slate{fib(3)}}
        child {node {\slate{fib(2)}}
            child {node {\slate{fib(1)}}}
            child {node {\slate{fib(0)}}}
        }
        child {node {\slate{fib(1)}}}
    };
\end{tikzpicture}
\caption{The tree-recursive process generated in computing \slate{fib(5)}.}
\label{fig:tree-recursive-fib}
\end{figure}

This procedure is instructive as a prototypical tree recursion, but it is a terrible way to compute Fibonacci numbers because it does so much redundant computation. Notice in Figure~\ref{fig:tree-recursive-fib} that the entire computation of \slate{fib(3)}---almost half the work---is duplicated. In fact, it is not hard to show that the number of times the procedure will compute \slate{fib(1)} or \slate{fib(0)} (the number of leaves in the above tree, in general) is precisely $\text{Fib}(n + 1)$. To get an idea of how bad this is, one can show that the value of $\text{Fib}(n)$ grows exponentially with $n$. More precisely (see Exercise 1.13), $\text{Fib}(n)$ is the closest integer to $\phi^n/\sqrt{5}$, where

$\phi = \frac{1 + \sqrt{5}}{2} \approx 1.6180$

is the golden ratio, which satisfies the equation

$\phi^2 = \phi + 1$.

Thus, the process uses a number of steps that grows exponentially with the input. On the other hand, the space required grows only linearly with the input, because we need keep track only of which nodes are above us in the tree at any point in the computation. In general, the number of steps required by a tree-recursive process will be proportional to the number of nodes in the tree, while the space required will be proportional to the maximum depth of the tree.

We can also formulate an iterative process for computing the Fibonacci numbers. The idea is to use a pair of integers $a$ and $b$, initialized to $\text{Fib}(1) = 1$ and $\text{Fib}(0) = 0$, and to repeatedly apply the simultaneous transformations

$a \leftarrow a + b$,
$b \leftarrow a$.

It is not hard to show that, after applying this transformation $n$ times, $a$ and $b$ will be equal, respectively, to $\text{Fib}(n + 1)$ and $\text{Fib}(n)$. Thus, we can compute Fibonacci numbers iteratively using the procedure

\begin{lstlisting}
def fib(n) = fib_iter(1, 0, n)

def fib_iter(a, b, count) =
    if count == 0 then
        b
    else
        fib_iter(a + b, a, count - 1)
\end{lstlisting}

This second method for computing $\text{Fib}(n)$ is a linear iteration. The difference in number of steps required by the two methods---one linear in $n$, one growing as fast as $\text{Fib}(n)$ itself---is enormous, even for small inputs.

One should not conclude from this that tree-recursive processes are useless. When we consider processes that operate on hierarchically structured data rather than numbers, we will find that tree recursion is a natural and powerful tool. But even in numerical operations, tree-recursive processes can be useful in helping us to understand and design programs. For instance, although the first \slate{fib} procedure is much less efficient than the second one, it is more straightforward, being little more than a translation into Slate of the definition of the Fibonacci sequence. To formulate the iterative algorithm required noticing that the computation could be recast as an iteration with three state variables.

\subsubsection{Example: Counting change}

It takes only a bit of cleverness to come up with the iterative Fibonacci algorithm. In contrast, consider the following problem: How many different ways can we make change of \$1.00, given half-dollars, quarters, dimes, nickels, and pennies? More generally, can we write a procedure to compute the number of ways to change any given amount of money?

This problem has a simple solution as a recursive procedure. Suppose we think of the types of coins available as arranged in some order. Then the following relation holds:

The number of ways to change amount $a$ using $n$ kinds of coins equals

\begin{itemize}
\item the number of ways to change amount $a$ using all but the first kind of coin, plus
\item the number of ways to change amount $a - d$ using all $n$ kinds of coins, where $d$ is the denomination of the first kind of coin.
\end{itemize}

To see why this is true, observe that the ways to make change can be divided into two groups: those that do not use any of the first kind of coin, and those that do. Therefore, the total number of ways to make change for some amount is equal to the number of ways to make change for the amount without using any of the first kind of coin, plus the number of ways to make change assuming that we do use the first kind of coin. But the latter number is equal to the number of ways to make change for the amount that remains after using a coin of the first kind.

Thus, we can recursively reduce the problem of changing a given amount to the problem of changing smaller amounts using fewer kinds of coins. Consider this reduction rule carefully, and convince yourself that we can use it to describe an algorithm if we specify the following degenerate cases:

\begin{itemize}
\item If $a$ is exactly 0, we should count that as 1 way to make change.
\item If $a$ is less than 0, we should count that as 0 ways to make change.
\item If $n$ is 0, we should count that as 0 ways to make change.
\end{itemize}

We can easily translate this description into a recursive procedure:

\begin{lstlisting}
def count_change(amount) = cc(amount, 5)

def cc(amount, kinds_of_coins) =
    if amount == 0 then
        1
    elif amount < 0 or kinds_of_coins == 0 then
        0
    else
        cc(amount, kinds_of_coins - 1) +
        cc(amount - first_denomination(kinds_of_coins), kinds_of_coins)

def first_denomination(kinds_of_coins) =
    if kinds_of_coins == 1 then 1
    elif kinds_of_coins == 2 then 5
    elif kinds_of_coins == 3 then 10
    elif kinds_of_coins == 4 then 25
    elif kinds_of_coins == 5 then 50
\end{lstlisting}

(The \slate{first_denomination} procedure takes as input the number of kinds of coins available and returns the denomination of the first kind. Here we are thinking of the coins as arranged in order from largest to smallest, but any order would do as well.) We can now answer our original question about changing a dollar:

\begin{lstlisting}
count_change(100)
\end{lstlisting}
\textit{292}

\slate{count_change} generates a tree-recursive process with redundancies similar to those in our first implementation of \slate{fib}. (It will take quite a while for that 292 to be computed.) On the other hand, it is not obvious how to design a better algorithm for computing the result, and we leave this problem as a challenge. The observation that a tree-recursive process may be highly inefficient but often easy to specify and understand has led people to propose that one could get the best of both worlds by designing a ``smart compiler'' that could transform tree-recursive procedures into more efficient procedures that compute the same result.

\begin{description}
\item[Exercise 1.11] A function $f$ is defined by the rule that

\begin{equation}
f(n) = \begin{cases}
n & \text{if } n < 3, \\
f(n - 1) + 2f(n - 2) + 3f(n - 3) & \text{if } n \geq 3.
\end{cases}
\end{equation}

Write a procedure that computes $f$ by means of a recursive process. Write a procedure that computes $f$ by means of an iterative process.

\item[Exercise 1.12] The following pattern of numbers is called Pascal's triangle.

\begin{verbatim}
    1
   1 1
  1 2 1
 1 3 3 1
1 4 6 4 1
   . . .
\end{verbatim}

The numbers at the edge of the triangle are all 1, and each number inside the triangle is the sum of the two numbers above it. Write a procedure that computes elements of Pascal's triangle by means of a recursive process.

\item[Exercise 1.13] Prove that $\text{Fib}(n)$ is the closest integer to $\phi^n/\sqrt{5}$, where $\phi = (1 + \sqrt{5})/2$. Hint: Let $\psi = (1 - \sqrt{5})/2$. Use induction and the definition of the Fibonacci numbers (see Section 1.2.2) to prove that $\text{Fib}(n) = (\phi^n - \psi^n)/\sqrt{5}$.
\end{description}

\subsection{Orders of Growth}

The previous examples illustrate that processes can differ considerably in the rates at which they consume computational resources. One convenient way to describe this difference is to use the notion of \emph{order of growth} to obtain a gross measure of the resources required by a process as the inputs become larger.

Let $n$ be a parameter that measures the size of the problem, and let $R(n)$ be the amount of resources the process requires for a problem of size $n$. In our previous examples we took $n$ to be the number for which the function is to be computed, but there are other possibilities. For instance, if our goal is to compute an approximation to the square root of a number, we might take $n$ to be the number of digits accuracy required. For matrix multiplication we might take $n$ to be the number of rows in the matrices. In general there are a number of properties of the problem with respect to which it will be desirable to analyze a given process. Similarly, $R(n)$ might measure the number of internal storage registers used, the number of elementary machine operations performed, and so on. In computers that do only a fixed number of operations at a time, the time required will be proportional to the number of elementary machine operations performed.

We say that $R(n)$ has order of growth $\Theta(f(n))$, written $R(n) = \Theta(f(n))$ (pronounced ``theta of $f(n)$''), if there are positive constants $k_1$ and $k_2$ independent of $n$ such that

$k_1 f(n) \leq R(n) \leq k_2 f(n)$

for any sufficiently large value of $n$. (In other words, for large $n$, the value $R(n)$ is sandwiched between $k_1 f(n)$ and $k_2 f(n)$.)

For instance, with the linear recursive process for computing factorial described in Section 1.2.1, the number of steps grows proportionally to the input $n$. Thus, the steps required for this process grows as $\Theta(n)$. We also saw that the space required grows as $\Theta(n)$. For the iterative factorial, the number of steps is still $\Theta(n)$ but the space is $\Theta(1)$---that is, constant. The tree-recursive Fibonacci computation requires $\Theta(\phi^n)$ steps and $\Theta(n)$ space, where $\phi$ is the golden ratio described in Section 1.2.2.

Orders of growth provide only a crude description of the behavior of a process. For example, a process requiring $n^2$ steps and a process requiring $1000n^2$ steps and a process requiring $3n^2 + 10n + 17$ steps all have $\Theta(n^2)$ order of growth. On the other hand, order of growth provides a useful indication of how we may expect the behavior of the process to change as we change the size of the problem. For a $\Theta(n)$ (linear) process, doubling the size will roughly double the amount of resources used. For an exponential process, each increment in problem size will multiply the resource utilization by a constant factor. In the remainder of Section 1.2 we will examine two algorithms whose order of growth is logarithmic, so that doubling the problem size increases the resource requirement by a constant amount.

\begin{description}
\item[Exercise 1.14] Draw the tree illustrating the process generated by the \slate{count_change} procedure of Section 1.2.2 in making change for 11 cents. What are the orders of growth of the space and number of steps used by this process as the amount to be changed increases?

\item[Exercise 1.15] The sine of an angle (specified in radians) can be computed by making use of the approximation $\sin x \approx x$ if $x$ is sufficiently small, and the trigonometric identity

$\sin x = 3 \sin \frac{x}{3} - 4 \sin^3 \frac{x}{3}$

to reduce the size of the argument of $\sin$. (For purposes of this exercise an angle is considered ``sufficiently small'' if its magnitude is not greater than 0.1 radians.) These ideas are incorporated in the following procedures:

\begin{lstlisting}
def cube(x) = x * x * x

def p(x) = 3 * x - 4 * cube(x)

def sine(angle) =
    if abs(angle) <= 0.1 then
        angle
    else
        p(sine(angle / 3.0))
\end{lstlisting}

\begin{enumerate}[label=\alph*.]
\item How many times is the procedure \slate{p} applied when \slate{sine(12.15)} is evaluated?
\item What is the order of growth in space and number of steps (as a function of $a$) used by the process generated by the \slate{sine} procedure when \slate{sine(a)} is evaluated?
\end{enumerate}
\end{description}

\subsection{Exponentiation}

Consider the problem of computing the exponential of a given number. We would like a procedure that takes as arguments a base $b$ and a positive integer exponent $n$ and computes $b^n$. One way to do this is via the recursive definition

\begin{align}
b^n &= b \cdot b^{n-1} \\
b^0 &= 1
\end{align}

which translates readily into the procedure

\begin{lstlisting}
def expt(b, n) =
    if n == 0 then
        1
    else
        b * expt(b, n - 1)
\end{lstlisting}

This is a linear recursive process, which requires $\Theta(n)$ steps and $\Theta(n)$ space. Just as with factorial, we can readily formulate an equivalent linear iteration:

\begin{lstlisting}
def expt(b, n) = expt_iter(b, n, 1)

def expt_iter(b, counter, product) =
    if counter == 0 then
        product
    else
        expt_iter(b, counter - 1, b * product)
\end{lstlisting}

This version requires $\Theta(n)$ steps and $\Theta(1)$ space.

We can compute exponentials in fewer steps by using successive squaring. For instance, rather than computing $b^8$ as

$b \cdot (b \cdot (b \cdot (b \cdot (b \cdot (b \cdot (b \cdot b))))))$

we can compute it using three multiplications:

\begin{align}
b^2 &= b \cdot b \\
b^4 &= b^2 \cdot b^2 \\
b^8 &= b^4 \cdot b^4
\end{align}

This method works fine for exponents that are powers of 2. We can also take advantage of successive squaring in computing exponentials in general if we use the rule

\begin{equation}
b^n = \begin{cases}
(b^{n/2})^2 & \text{if } n \text{ is even} \\
b \cdot b^{n-1} & \text{if } n \text{ is odd}
\end{cases}
\end{equation}

We can express this method as a procedure:

\begin{lstlisting}
def fast_expt(b, n) =
    if n == 0 then 1
    elif even(n) then square(fast_expt(b, n / 2))
    else b * fast_expt(b, n - 1)

def even(n) = (n % 2) == 0

def square(x) = x * x
\end{lstlisting}

The process evolved by \slate{fast_expt} grows logarithmically with $n$ in both space and number of steps. To see this, observe that computing $b^{2n}$ using \slate{fast_expt} requires only one more multiplication than computing $b^n$. The size of the exponent we can compute therefore doubles (approximately) with every new multiplication we are allowed. Thus, the number of multiplications required for an exponent of $n$ grows as $\Theta(\log n)$. The process has $\Theta(\log n)$ growth.

The difference between $\Theta(\log n)$ growth and $\Theta(n)$ growth becomes striking as $n$ becomes large. For example, \slate{fast_expt} for $n = 1000$ requires only 14 multiplications. It is also possible to use the idea of successive squaring to devise an iterative algorithm that computes exponentials with a logarithmic number of steps (see Exercise 1.16), although, as is often the case with iterative algorithms, this is not written down so straightforwardly as the recursive algorithm.

\begin{description}
\item[Exercise 1.16] Design a procedure that evolves an iterative exponentiation process that uses a logarithmic number of steps, as does \slate{fast_expt}. (Hint: Using the observation that $(b^{n/2})^2 = (b^2)^{n/2}$, keep, along with the exponent $n$ and the base $b$, an additional state variable $a$, and define the state transformation in such a way that the product $a b^n$ is unchanged from state to state. At the beginning of the process $a$ is taken to be 1, and the answer is given by the value of $a$ at the end of the process. In general, the technique of defining an \emph{invariant quantity} that remains unchanged from state to state is a powerful way to think about the design of iterative algorithms.)

\item[Exercise 1.17] The exponentiation algorithms in this section are based on performing exponentiation by means of repeated multiplication. In a similar way, one can perform integer multiplication by means of repeated addition. The following multiplication procedure (in which it is assumed that our language can only add, not multiply) is analogous to the \slate{expt} procedure:

\begin{lstlisting}
def mul(a, b) =
    if b == 0 then
        0
    else
        a + mul(a, b - 1)
\end{lstlisting}

This algorithm takes a number of steps that is linear in $b$. Now suppose we include, together with addition, operations \slate{double}, which doubles an integer, and \slate{halve}, which divides an (even) integer by 2. Using these, design a multiplication procedure analogous to \slate{fast_expt} that uses a logarithmic number of steps.

\item[Exercise 1.18] Using the results of Exercises 1.16 and 1.17, devise a procedure that generates an iterative process for multiplying two integers in terms of adding, doubling, and halving and uses a logarithmic number of steps.

\item[Exercise 1.19] There is a clever algorithm for computing the Fibonacci numbers in a logarithmic number of steps. Recall the transformation of the state variables $a$ and $b$ in the \slate{fib_iter} process of Section 1.2.2: $a \leftarrow a + b$ and $b \leftarrow a$. Call this transformation $T$, and observe that applying $T$ over and over again $n$ times, starting with 1 and 0, produces the pair $\text{Fib}(n + 1)$ and $\text{Fib}(n)$. In other words, the Fibonacci numbers are produced by applying $T^n$, the $n$th power of the transformation $T$, starting with the pair $(1,0)$. Now consider $T$ to be the special case of $p = 0$ and $q = 1$ in a family of transformations $T_{pq}$, where $T_{pq}$ transforms the pair $(a,b)$ according to $a \leftarrow bq + aq + ap$ and $b \leftarrow bp + aq$. Show that if we apply such a transformation $T_{pq}$ twice, the effect is the same as using a single transformation $T_{p'q'}$ of the same form, and compute $p'$ and $q'$ in terms of $p$ and $q$. This gives us an explicit way to square these transformations, which we can use to compute $T^n$ using successive squaring, as in the \slate{fast_expt} procedure. Put this all together to complete the following procedure, which runs in a logarithmic number of steps:

\begin{lstlisting}
def fib(n) = fib_iter(1, 0, 0, 1, n)

def fib_iter(a, b, p, q, count) =
    if count == 0 then
        b
    elif even(count) then
        fib_iter(a,
                 b,
                 ???,  \\ compute p'
                 ???,  \\ compute q'
                 count / 2)
    else
        fib_iter(b * q + a * q + a * p,
                 b * p + a * q,
                 p,
                 q,
                 count - 1)
\end{lstlisting}
\end{description}

\subsection{Greatest Common Divisors}

The greatest common divisor (GCD) of two integers $a$ and $b$ is defined to be the largest integer that divides both $a$ and $b$ with no remainder. For example, the GCD of 16 and 28 is 4. In Chapter 2, when we investigate how to implement rational-number arithmetic, we will need a procedure that computes the GCD of two integers. One way to find the GCD of two integers is to factor them and search for common factors, but there is a famous algorithm that is much more efficient.

The idea of the algorithm is based on the observation that, if $r$ is the remainder when $a$ is divided by $b$, then the common divisors of $a$ and $b$ are precisely the same as the common divisors of $b$ and $r$. Thus, we can use the equation

$\gcd(a,b) = \gcd(b,r)$

to successively reduce the problem of computing a GCD to the problem of computing the GCD of smaller and smaller pairs of integers. For example,

\begin{align}
\gcd(206,40) &= \gcd(40,6) \\
&= \gcd(6,4) \\
&= \gcd(4,2) \\
&= \gcd(2,0) \\
&= 2
\end{align}

reduces $\gcd(206,40)$ to $\gcd(2,0)$, which is 2. It is possible to show that starting with any two positive integers, this repeated reduction will always eventually produce a pair where the second number is 0. Then the GCD is the other number in the pair. This method for computing the GCD is known as Euclid's Algorithm.

It is easy to express Euclid's Algorithm as a procedure:

\begin{lstlisting}
def gcd(a, b) =
    if b == 0 then
        a
    else
        gcd(b, a % b)
\end{lstlisting}

This generates an iterative process, whose number of steps grows as the logarithm of the numbers involved.

The fact that the number of steps required by Euclid's Algorithm has logarithmic growth provides another example of the value of expressing programs as recursive procedures. Although the procedure is specified in terms of a recursive call to itself, the underlying process is actually iterative (see the discussion in Section 1.2.1). It is quite easy to verify this by noting that all the procedure does is transform the problem parameters according to a fixed rule. The number of steps required by Euclid's Algorithm to compute the GCD of two integers is proportional to the logarithm of the smaller of the two integers.

\begin{description}
\item[Exercise 1.20] The process that a procedure generates is of course dependent on the rules used by the interpreter. As an example, consider the iterative \slate{gcd} procedure given above. Suppose we were to interpret this procedure using normal-order evaluation, as discussed in Section 1.1.5. (The normal-order-evaluation rule is to evaluate the operator and operands and then apply the resulting procedure.) Using the substitution method (for normal order), illustrate the process generated in evaluating \slate{gcd(206, 40)} and indicate the \slate{remainder} operations that are actually performed. How many \slate{remainder} operations are actually performed in the normal-order evaluation of \slate{gcd(206, 40)}? In the applicative-order evaluation?
\end{description}

\subsection{Example: Testing for Primality}

This section describes two methods for checking the primality of an integer $n$, one with order of growth $\Theta(\sqrt{n})$, and a ``probabilistic'' algorithm with order of growth $\Theta(\log n)$. The exercises at the end of this section suggest programming projects based on these algorithms.

\subsubsection{Searching for divisors}

Since ancient times, mathematicians have been fascinated by problems concerning prime numbers, and many people have worked on the problem of determining ways to test if numbers are prime. One way to test if a number is prime is to find the number's divisors. The following program finds the smallest integral divisor (greater than 1) of a given number $n$. It does this in a straightforward way, by testing $n$ for divisibility by successive integers starting with 2.

\begin{lstlisting}
def smallest_divisor(n) = find_divisor(n, 2)

def find_divisor(n, test_divisor) =
    if square(test_divisor) > n then
        n
    elif divides(test_divisor, n) then
        test_divisor
    else
        find_divisor(n, test_divisor + 1)

def divides(a, b) = (b % a) == 0
\end{lstlisting}

We can test whether a number is prime as follows: $n$ is prime if and only if $n$ is its own smallest divisor.

\begin{lstlisting}
def prime(n) = n == smallest_divisor(n)
\end{lstlisting}

The end test for \slate{find_divisor} is based on the fact that if $n$ is not prime it must have a divisor less than or equal to $\sqrt{n}$. This means that the algorithm need only test divisors between 1 and $\sqrt{n}$. Consequently, the number of steps required to identify $n$ as prime will have order of growth $\Theta(\sqrt{n})$.

\subsubsection{The Fermat test}

The $\Theta(\log n)$ primality test is based on a result from number theory known as Fermat's Little Theorem.

\textbf{Fermat's Little Theorem:} If $n$ is a prime number and $a$ is any positive integer less than $n$, then $a$ raised to the $n$th power is congruent to $a$ modulo $n$.

(Two numbers are said to be \emph{congruent modulo} $n$ if they both have the same remainder when divided by $n$. The remainder of a number $a$ when divided by $n$ is also referred to as the \emph{remainder of $a$ modulo $n$}, or simply as \emph{$a$ modulo $n$}.)

If $n$ is not prime, then, in general, most of the numbers $a < n$ will not satisfy the above relation. This leads to the following algorithm for testing primality: Given a number $n$, pick a random number $a < n$ and compute the remainder of $a^n$ modulo $n$. If the result is not equal to $a$, then $n$ is certainly not prime. If it is $a$, then chances are good that $n$ is prime. Now pick another random number $a$ and test it with the same method. If it also satisfies the equation, then we can be even more confident that $n$ is prime. By trying more and more values of $a$, we can increase our confidence in the result. This algorithm is known as the Fermat test.

To implement the Fermat test, we need a procedure that computes the exponential of a number modulo another number:

\begin{lstlisting}
def expmod(base, exp, m) =
    if exp == 0 then
        1
    elif even(exp) then
        square(expmod(base, exp / 2, m)) % m
    else
        (base * expmod(base, exp - 1, m)) % m
\end{lstlisting}

This is very similar to the \slate{fast_expt} procedure of Section 1.2.4. It uses successive squaring, so that the number of steps grows logarithmically with the exponent.

The Fermat test is performed by choosing at random a number $a$ between 1 and $n - 1$ inclusive and checking whether the remainder modulo $n$ of the $n$th power of $a$ is equal to $a$. The random number $a$ is chosen using the procedure \slate{random}, which we assume is included as a primitive in Slate. \slate{random} returns a nonnegative integer less than its integer input. Hence, to obtain a random number between 1 and $n - 1$, we call \slate{random(n - 1)} and add 1:

\begin{lstlisting}
def fermat_test(n) =
    def try_it(a) = (expmod(a, n, n) == a)
    try_it(1 + random(n - 1))
\end{lstlisting}

The following procedure runs the test a given number of times, as specified by a parameter. Its value is true if the test succeeds every time, and false otherwise.

\begin{lstlisting}
def fast_prime(n, times) =
    if times == 0 then
        true
    elif fermat_test(n) then
        fast_prime(n, times - 1)
    else
        false
\end{lstlisting}

\subsubsection{Probabilistic methods}

The Fermat test differs in character from most familiar algorithms, in which one computes an answer that is guaranteed to be correct. Here, the answer obtained is only probably correct. More precisely, if $n$ ever fails the Fermat test, we can be certain that $n$ is not prime. But the fact that $n$ passes the test, while an extremely strong indication, is still not a guarantee that $n$ is prime. What we would like to say is that for any number $n$, if we perform the test enough times and find that $n$ always passes the test, then the probability of error in our primality test can be made as small as we like.

Unfortunately, this assertion is not quite correct. There do exist numbers that fool the Fermat test: numbers $n$ that are not prime and yet have the property that $a^n$ is congruent to $a$ modulo $n$ for all integers $a < n$. Such numbers are extremely rare, so the Fermat test is quite reliable in practice. There are variants of the Fermat test that cannot be fooled. In these tests, as with the Fermat method, one tests the primality of an integer $n$ by choosing a random integer $a < n$ and checking some condition (easy to compute) involving $a$ and $n$. The difference is that these tests can prove that an integer is not prime without being subject to the error of false testimony to primality. (See Exercise 1.28.)

On the other hand, in contrast to algorithms whose results are either correct or incorrect, probabilistic algorithms can be highly useful in some applications. A prime example is the use of probabilistic primality testing in implementing a code-breaking system based on the RSA cryptographic algorithm. Numbers of several hundred digits are routinely tested for primality using probabilistic methods and then used as keys for RSA. The probabilities of error in such applications can be estimated, and can be made smaller than the probability of the most unlikely physical events (such as cosmic rays corrupting a computer memory) and well below any reasonable threshold for practical use.

\begin{description}
\item[Exercise 1.21] Use the \slate{smallest_divisor} procedure to find the smallest divisor of each of the following numbers: 199, 1999, 19999.

\item[Exercise 1.22] Most Slate implementations include a primitive called \slate{runtime} that returns an integer that specifies the amount of time the system has been running (measured, for example, in microseconds). The following \slate{timed_prime_test} procedure, when called with an integer $n$, prints $n$ and checks to see if $n$ is prime. If $n$ is prime, the procedure prints three asterisks followed by the amount of time used in performing the test.

\begin{lstlisting}
def timed_prime_test(n) =
    newline()
    display(n)
    start_prime_test(n, runtime())

def start_prime_test(n, start_time) =
    if prime(n) then
        report_prime(runtime() - start_time)

def report_prime(elapsed_time) =
    display(" *** ")
    display(elapsed_time)
\end{lstlisting}

Using this procedure, write a procedure \slate{search_for_primes} that checks the primality of consecutive odd integers in a specified range. Use your procedure to find the three smallest primes larger than 1000; larger than 10,000; larger than 100,000; larger than 1,000,000. Note the time needed to test each prime. Since the testing algorithm has order of growth $\Theta(\sqrt{n})$, you should expect that testing for primes around 10,000 should take about $\sqrt{10}$ times as long as testing for primes around 1000. Do your timing data bear this out? How well do the data for 100,000 and 1,000,000 support the $\Theta(\sqrt{n})$ prediction? Is your result compatible with the notion that programs on your machine run in time proportional to the number of steps required for the computation?

\item[Exercise 1.23] The \slate{smallest_divisor} procedure shown at the start of this section does lots of needless testing: After it checks to see if the number is divisible by 2 there is no point in checking to see if it is divisible by any larger even number. This suggests that the values used for \slate{test_divisor} should not be 2, 3, 4, 5, 6, \ldots, but rather 2, 3, 5, 7, 9, \ldots. To implement this change, define a procedure \slate{next} that returns 3 if its input is equal to 2 and otherwise returns its input plus 2. Modify the \slate{smallest_divisor} procedure to use \slate{(next test_divisor)} instead of \slate{(test_divisor + 1)}. With \slate{timed_prime_test} incorporating this modified version of \slate{smallest_divisor}, run the test for each of the 12 primes found in Exercise 1.22. Since this modification halves the number of test steps, you should expect it to run about twice as fast. Is this expectation confirmed? If not, what is the observed ratio of the speeds of the two algorithms, and how do you explain the fact that it is different from 2?

\item[Exercise 1.24] Modify the \slate{timed_prime_test} procedure of Exercise 1.22 to use \slate{fast_prime} (the Fermat method) instead of \slate{prime}, and test each of the 12 primes you found in that exercise. Since the Fermat test has $\Theta(\log n)$ growth, how would you expect the time to test primes near 1,000,000 to compare with the time needed to test primes near 1000? Do your data bear this out? Can you explain any discrepancy you find?

\item[Exercise 1.25] Alyssa P. Hacker complains that we went to a lot of extra work in writing \slate{expmod}. After all, she says, since we already know how to compute exponentials, we could have simply written

\begin{lstlisting}
def expmod(base, exp, m) = (fast_expt(base, exp)) % m
\end{lstlisting}

Is she correct? Would this procedure serve as well for our fast prime tester? Explain.

\item[Exercise 1.26] Louis Reasoner is having great difficulty doing Exercise 1.24. His \slate{fast_prime} test seems to run more slowly than his \slate{prime} test. Louis calls his friend Eva Lu Ator over to help. When they examine Louis's code, they find that he has rewritten the \slate{expmod} procedure to use an explicit multiplication, rather than calling \slate{square}:

\begin{lstlisting}
def expmod(base, exp, m) =
    if exp == 0 then
        1
    elif even(exp) then
        (expmod(base, exp / 2, m) * expmod(base, exp / 2, m)) % m
    else
        (base * expmod(base, exp - 1, m)) % m
\end{lstlisting}

``I don't see what difference that could make,'' says Louis. ``I do.'' says Eva. ``By writing the procedure like that, you have transformed the $\Theta(\log n)$ process into a $\Theta(n)$ process.'' Explain.

\item[Exercise 1.27] Demonstrate that the Carmichael numbers---561, 1105, 1729, 2465, 2821, and 6601---really do fool the Fermat test. That is, write a procedure that takes an integer $n$ and tests whether $a^n$ is congruent to $a$ modulo $n$ for every $a < n$, and try your procedure on the given Carmichael numbers.

\item[Exercise 1.28] One variant of the Fermat test that cannot be fooled is called the \emph{Miller-Rabin test} (Miller 1976; Rabin 1980). This starts from an alternate form of Fermat's Little Theorem, which states that if $n$ is a prime number and $a$ is any positive integer less than $n$, then $a$ raised to the $(n - 1)$st power is congruent to 1 modulo $n$. To test the primality of a number $n$ by the Miller-Rabin test, we pick a random number $a < n$ and raise $a$ to the $(n - 1)$st power modulo $n$ using the \slate{expmod} procedure. However, whenever we perform the squaring step in \slate{expmod}, we check to see if we have discovered a ``nontrivial square root of 1 modulo $n$,'' that is, a number not equal to 1 or $n - 1$ whose square is equal to 1 modulo $n$. It is possible to prove that if such a nontrivial square root of 1 exists, then $n$ is not prime. It is also possible to prove that if $n$ is an odd number that is not prime, then, for at least half the numbers $a < n$, computing $a^{n-1}$ in this way will reveal a nontrivial square root of 1 modulo $n$. (This is why the Miller-Rabin test cannot be fooled.) Modify the \slate{expmod} procedure to signal if it discovers a nontrivial square root of 1, and use this to implement the Miller-Rabin test with a procedure analogous to \slate{fermat_test}. Check your procedure by testing it on some known primes and non-primes. Hint: One convenient way to make \slate{expmod} signal is to have it return 0.
\end{description}